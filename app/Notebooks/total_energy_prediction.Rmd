---
title: "Prediction of Total Electrical Net Generation From Coal"
author: "John Grando"
date: "October 19, 2020"
output: 
  html_document:
    css: "CSS/main.css"

---

<strong>[Home Page](https://john-grando.github.io/)</strong>

In this analysis, we take a look at the aggregated U.S. net electrical generation, and how that power is produced.  The preliminary steps will show the breakdown of various fuel sources and then we will move into a quick forecast of coal consumption within the next few years.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  dev.args=list(bg = 'transparent'),
  fig.align='center')
```

```{r warning=FALSE}
suppressWarnings(suppressMessages(library(tidyverse)))
suppressWarnings(suppressMessages(library(RColorBrewer)))
suppressWarnings(suppressMessages(library(kableExtra)))
suppressWarnings(suppressMessages(library(gridExtra)))
suppressWarnings(suppressMessages(library(forecast)))
suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(sparklyr)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(tidyr)))
```

First, we will load cleansed data from a previous ETL phase that was stored in hadoop.  This information is provided from the bulk downloaded files available at the [U.S. Energy Information Administration](https://www.eia.gov/opendata/bulkfiles.php).

```{r}
sc <- sparklyr::spark_connect(
  master = 'local[2]', 
  spark_home = Sys.getenv("SPARK_HOME"))
total_energy_df <- sparklyr::spark_read_parquet(
  sc = sc, 
  name = 'total_energy_df', 
  path = "hdfs://localhost:9000/Processed/TotalEnergyDF")
total_energy_df <- select(
    total_energy_df, 
    "date", 
    "name", 
    "value", 
    "units") %>% 
  mutate(
    date = as.Date(date),
    value = as.numeric(value)) %>% 
  arrange(date)
```

We will cut off any measurements older than 1990, as many records are missing for certain categories, and the does not appear to be useful in its current form.  After a quick check, we see that there are no null values in the fields we selected.

```{r}
electricity_net_generation_df <- total_energy_df %>% 
  dplyr::filter(date >= as.Date("1990-01-01") & date < as.Date("2020-01-01")) %>% 
  dplyr::filter(name %rlike% "(Electricity Net Generation From.*All Sectors$)")
```

```{r eval=FALSE}
electricity_net_generation_df %>% 
  dplyr::filter(is.na(value) | value == '') %>% 
  dplyr::count(name = "null fields") %>% 
  kable('html') %>% 
  kable_styling(
    bootstrap_options = 'striped',
    full_width = FALSE,
    position = "center",
    font_size = 20)
```


```{r}
date_limit_df <- electricity_net_generation_df %>% 
  group_by() %>% 
  dplyr::summarize(
    min_date = min(date), 
    max_date = max(date)) %>% 
  sparklyr::collect()
```


```{r}
electricity_net_generation_pivot_df <- electricity_net_generation_df %>% 
  dplyr::mutate(
    name = regexp_replace(name, '^Electricity Net Generation From (.*), All Sectors$', 'eng_$1'),
    name = tolower(name)
  ) %>% 
  mutate(
    name = regexp_replace(name, ' ', '_')
  ) %>% 
  sdf_pivot(
    formula = date ~ name, 
    fun.aggregate = list(value = "mean"))

```

And now, we can take a first look at the net generation by fuel type.  It's immediately obvious that coal, natural gas, and nuclear power make up the major portion of consumed fuels, so we will group all others into a single category for simplicity.

```{r fig.width=12}
color_values <- colorspace::diverge_hcl(n = 12, h = c(180, 260), c = c(0, 260), l = c(0,80), power = 2)
color_values_big <- colorspace::diverge_hcl(n = 20, h = c(180, 260), c = c(0, 260), l = c(0,100), power = 2)

electricity_net_generation_pivot_melted_df <- electricity_net_generation_pivot_df %>% 
    collect() %>% 
    gather(key = "id", value = 'val', -c(date))

plot_theme <- theme(plot.title = element_text(hjust = 0.5, size = 25),
        plot.subtitle = element_text(hjust = 0.5, size = 14),
        strip.text = element_text(size = 20),
        plot.background = element_rect(fill = "lightgrey"), 
        panel.background = element_rect(fill = "white"), 
        panel.grid.major.x = element_line(color = "lightgrey"), 
        panel.grid.major.y = element_line(color = "lightgrey"), 
        axis.text = element_text(size=14, color = "grey1"), 
        axis.title = element_text(size=16, color = "grey1"))

ggplot(
  data = electricity_net_generation_pivot_melted_df, 
  mapping = aes(x = date, y = val, color = id)) +
  geom_line() +
  scale_color_manual(values = color_values) +
  labs(x = "Date", y = "Million kWh") +
  ggtitle("Electricity Net Generation") +
  plot_theme
```

```{r}
min_cutoff_date <- '2009-01-01'
electricity_net_generation_pivot_reduced_df <- electricity_net_generation_df %>% 
  filter(date > as.Date(min_cutoff_date)) %>% 
  dplyr::mutate(
    name = regexp_replace(name, '^Electricity Net Generation From (.*), All Sectors$', 'eng_$1'),
    name = tolower(name)) %>% 
  mutate(
    name = regexp_replace(name, ' ', '_')
  ) %>% 
  mutate(
    name = ifelse(
      name %in% c(
        "eng_coal", 
        "eng_natural_gas", 
        "eng_nuclear_electric_power"), 
      name, 
      "eng_other")
  ) %>% 
  sdf_pivot(
    formula = date ~ name, 
    fun.aggregate = list(value = "mean"))

date_limit_reduced_df <- electricity_net_generation_pivot_reduced_df %>% 
  group_by() %>% 
  dplyr::summarize(
    min_date = min(date), 
    max_date = max(date)) %>% 
  sparklyr::collect()
```

Now we can see that in general the generation from coal has been on a slow decline with natural gas doing the opposite and nuclear electric power holding at a steady output.

```{r fig.width=12}
electricity_net_generation_pivot_melted_df <- electricity_net_generation_pivot_reduced_df %>% 
    collect() %>% 
    gather(key = "id", value = 'val', -c(date))
ggplot(
  data = electricity_net_generation_pivot_melted_df, 
  mapping = aes(x = date, y = val, color = id)) +
  geom_line() +
  scale_color_manual(values = color_values) +
  labs(x = "Date", y = "Million kWh") +
  ggtitle("Electricity Net Generation - Reduced Categories") +
  plot_theme
```

So, let's see if we can predict the future of coal consumption.  Note, this is very much a simple analysis and there are many factors that contribute to the various fuels usages, inclding political and economical.  For this analysis, we will try to fit the time series to an ARIMA model, given how flexible it can be in hyperparamter testing.  Also, predictor variables can be passed to these models via the `xreg` option, which we hope will help the model accuracy.

```{r}
eng_coal_ts <- ts(
  data = electricity_net_generation_pivot_reduced_df %>%  
    arrange(date) %>% 
    select("eng_coal") %>% 
    collect() %>% 
    pull("eng_coal"), 
  start = c(
    as.numeric(format(date_limit_reduced_df["min_date"][[1]], "%Y")), 
    as.numeric(format(date_limit_reduced_df["min_date"][[1]], "%m"))),
  end = c(
    as.numeric(format(date_limit_reduced_df["max_date"][[1]], "%Y")), 
    as.numeric(format(date_limit_reduced_df["max_date"][[1]], "%m"))),
  frequency = 12)
```

From the seasonal plot wee see a rather consistent pattern of peaks and valleys based on the month. The ACF plot is decaying, indicating that an arima differencing model may be appropriate, or could benefit from enabling drift.  The PACF plot show spikes at periodic lags, suggesting that differencing and/or moving average models may be appropriate as well.

```{r fig.width=12}
ggseasonplot(eng_coal_ts, polar=TRUE) +
  scale_color_manual(values=color_values_big) +
  ggtitle("Seasonal Coal Generation") +
  plot_theme
```

```{r fig.width=12}
autoplot(eng_coal_ts, series='original') + 
  labs(x = "Year", y = "Million kWh") +
  ggtitle("Electricity Net Generation From Coal") +
  scale_color_manual(values=color_values) +
  plot_theme
```

```{r fig.width=12}
acf_plot <- ggAcf(eng_coal_ts) + 
  plot_theme +
  ggtitle("ACF")
pacf_plot <- ggPacf(eng_coal_ts) +
  plot_theme +
  ggtitle("PACF")
grid.arrange(acf_plot, pacf_plot, ncol=2)
```

In order to try and create the most useful model, we will try to use the natural gas power generation values as predictors for the ARIMA model as well.  Some variations on this data were tested, namely the rolling mean value over the last 12 months and lagged generation values.  Only one predictor was selected to avoid any multicollinearity issues.  Also, due to the COVID pandemic, we will leave out the 2020 data from training and testing (that will likely be a different analysis in the future).

```{r}
prediction_length <- 48
eng_coal_train_ts <- subset(eng_coal_ts, start = 1, end = length(eng_coal_ts) - prediction_length)
eng_coal_test_ts <- subset(eng_coal_ts, start = length(eng_coal_ts) - prediction_length)

# Previous attempts used the raw net generation values from other sources.
# Howver, using the cumulative means provides a simpler comparison and smoother
#predictor for fitting
#p_xreg <- matrix(
#  c(
#    electricity_net_generation_pivot_reduced_df %>% select('eng_natural_gas') %>% pull(),
#    electricity_net_generation_pivot_reduced_df %>% select('eng_nuclear_electric_power') %>% pull()),
#  ncol=2, 
#  byrow = FALSE
#)

#using rolling median
p_rollmean_df <- electricity_net_generation_pivot_df %>% 
  arrange(date) %>% 
  select(
    date, 
    eng_natural_gas) %>%  
#    eng_nuclear_electric_power) %>% 
  collect() %>% 
  mutate(
    #eng_natural_gas_rm = zoo::rollmeanr(lag(eng_natural_gas), k = 13, fill = NA),
    eng_natural_gas = lag(eng_natural_gas)
 #   eng_nuclear_electric_power = zoo::rollmeanr(lag(eng_nuclear_electric_power), k = 13, fill = NA)
  ) %>% 
  filter(date > as.Date(min_cutoff_date)) %>% 
  select(-date) 
  
  
#only pick one predictor to avoid collinearity issues
p_xreg <- matrix(
  c(
    #p_rollmean_df %>% select('eng_natural_gas_rm') %>% pull()),
    p_rollmean_df %>% select('eng_natural_gas') %>% pull()),
    #p_rollmean_df %>% select('eng_nuclear_electric_power') %>% pull()),
  ncol=1, 
  byrow = FALSE
)

p_xreg_train <- matrix(p_xreg[1:length(eng_coal_train_ts),], ncol = dim(p_rollmean_df)[2])
p_xreg_test <- matrix(p_xreg[length(eng_coal_train_ts):length(eng_coal_ts),], ncol = dim(p_rollmean_df)[2])

#export for hyperparameter testing
save(eng_coal_train_ts, p_xreg_train, file = paste("../RFiles/Data/total_energy_coal_data.RData", sep="/"))
```

As a first step, we will evaluate what happens when we let `auto.arima` pick the model for us.  We will hold out the last `r prediction_length` predictions for later testing.  Non-seasonal differencing was not selected to be used; however, it appears that drift was enabled with a seasonal moving average.  The regressors may have something to do with affecting which model was selected; however the standard error associated with each predictor does not indicate a strong influence.  Given all that, do see that the box test has given us promising results that the residuals are not correlated.  

```{r}
auto_fit <- auto.arima(eng_coal_train_ts, xreg = p_xreg_train)
checkresiduals(auto_fit)
summary(auto_fit)
```

Now, let's give our own guess a shot where we used the [time series cross validation function](https://otexts.com/fpp2/accuracy.html) (tsCV) from the [forecast](https://github.com/robjhyndman/forecast) package.  This training focused on selecting the best model at the furthest prediction value (`r prediction_length`).  Note, I had to make a series of edits to make this function operate when provided with the `xreg` option.  Notes on these edits can be found [here](https://github.com/john-grando/eia-data-analysis/blob/master/app/Notebooks/tsCV_analysis.md).

The results of the hyperparameter training also included a drift term, but this time added a non-seasonal moving average term..  However, during testing, it appeared that the inclusion of the drift parameter was affecting the significance of the predictor.  By allowing the model the slowly decrease via the drift parameter, the inverse correlation between the natural gas consumption seemed to go unnoticed.  Therefore, the selected model, and a 'no drift' model were kept for analysis.  

```{r}
hyper_fit <- Arima(
  eng_coal_train_ts, 
  order=c(2,0,1),
  seasonal=c(1,0,2),
  lambda = NULL,
  include.drift = TRUE,
  xreg=p_xreg_train)

checkresiduals(hyper_fit)
summary(hyper_fit)
```

```{r}
hyper_fit_no_drift <- Arima(
  eng_coal_train_ts, 
  order=c(2,0,1),
  seasonal=c(2,1,1),
  lambda = NULL,
  include.drift = FALSE,
  xreg=p_xreg_train)

checkresiduals(hyper_fit_no_drift)
summary(hyper_fit_no_drift)
```

Now that the two complex models have been trained, we can test them against each other as well as a few other simpler prediction methods for comparison.  Specifically, we will add a seasonal naive predictor and an `auto.arima` model using no predictors

```{r}
auto_simple_fit <- auto.arima(eng_coal_train_ts)
checkresiduals(auto_simple_fit)
summary(auto_simple_fit)
```

Our selected hyperparameter model performs the best; however, not by much.  Also, as showin in the summary plots above, the predictor variables appear to have a very weak influence on the model and barely help the hyperparameter trained model fit outperform the simple ARIMA model.

```{r}
hyper_fit_df <- hyper_fit %>% 
  forecast(h = length(eng_coal_test_ts), xreg=p_xreg_test) %>% 
  accuracy(eng_coal_test_ts) %>% 
  data.frame() %>%  
  tibble::rownames_to_column(var = "model") %>% 
  mutate(model = paste("Hyper",model))
hyper_fit_no_drift_df <- hyper_fit_no_drift %>% 
  forecast(h = length(eng_coal_test_ts), xreg=p_xreg_test) %>% 
  accuracy(eng_coal_test_ts) %>% 
  data.frame() %>%  
  tibble::rownames_to_column(var = "model") %>% 
  mutate(model = paste("Hyper No Drift",model))
auto_fit_df <- auto_fit %>% 
  forecast(h = length(eng_coal_test_ts), xreg=p_xreg_test) %>% 
  accuracy(eng_coal_test_ts) %>% 
  data.frame() %>%  
  tibble::rownames_to_column(var = "model") %>% 
  mutate(model = paste("Auto",model))
auto_simple_fit_df <- auto_simple_fit %>% 
  forecast(h = length(eng_coal_test_ts)) %>% 
  accuracy(eng_coal_test_ts) %>% 
  data.frame() %>%  
  tibble::rownames_to_column(var = "model") %>% 
  mutate(model = paste("Auto Simple",model))
snaive_fit_df <- snaive(y = eng_coal_train_ts, h = length(eng_coal_test_ts)) %>% 
  accuracy(eng_coal_test_ts)  %>% 
  data.frame() %>%  
  tibble::rownames_to_column(var = "model") %>% 
  mutate(model = paste("Seasonal Naive",model))

hyper_fit_df %>% rbind(
  hyper_fit_no_drift_df,
  auto_fit_df,
  auto_simple_fit_df,
  snaive_fit_df
) %>% 
  filter(grepl('test', model, ignore.case = TRUE)) %>% 
  arrange(RMSE) %>% 
  kable('html') %>% 
  kable_styling(bootstrap_options = "striped", position = 'center', full_width = TRUE)
```

Now, we can plot the predictions of the four models we created in this analysis.

```{r fig.width=12, warning=FALSE, message=FALSE}
#Adjust future predictors

auto_simple_forecast <- forecast(
    auto_simple_fit,
    h = prediction_length)

auto_forecast <- forecast(
    auto_fit,
    h = prediction_length, 
    xreg=p_xreg_test)

hyper_forecast <- forecast(
  hyper_fit,
  h = prediction_length, 
  xreg=p_xreg_test
)

hyper_no_drift_forecast <- forecast(
  hyper_fit_no_drift,
  h = prediction_length, 
  xreg=p_xreg_test
)

base_plot <- autoplot(
  ts(
      c(p_xreg[,1]),
      start = c(
    as.numeric(format(date_limit_reduced_df["min_date"][[1]], "%Y")), 
    as.numeric(format(date_limit_reduced_df["min_date"][[1]], "%m"))),
      frequency = 12),
    series = 'mean natural gas') +
  autolayer(
    ts(
      c(p_xreg),
      start = c(
    as.numeric(format(date_limit_reduced_df["min_date"][[1]], "%Y")), 
    as.numeric(format(date_limit_reduced_df["min_date"][[1]], "%m"))),
      frequency = 12),
    series = 'actual natural gas') + 
  autolayer(
    snaive(y = eng_coal_train_ts, length(eng_coal_test_ts)),
    PI = FALSE,
    series = "naive") +
  labs(x="Date", y="Million kWh") +
  scale_x_continuous(
    labels=function(x){as.integer(x)},
    limits=c(
      as.numeric(format(date_limit_reduced_df["max_date"][[1]], "%Y"))-5, 
      as.numeric(format(date_limit_reduced_df["max_date"][[1]], "%Y"))+1),
    breaks = seq(
      as.numeric(format(date_limit_reduced_df["max_date"][[1]], "%Y"))-5, 
      as.numeric(format(date_limit_reduced_df["max_date"][[1]], "%Y"))+1, 
      1)) +
  autolayer(
    eng_coal_ts,
    series = "actual coal"
  ) +
  plot_theme +
  scale_color_viridis_d()

base_plot +
  autolayer(auto_simple_forecast, alpha = 0.3) +
  autolayer(
    auto_simple_forecast,
    series = "Prediction",
    PI = FALSE) +
  ggtitle(
    'Auto Arima Model - No Predictors', 
    subtitle = as.character(auto_simple_fit))

base_plot +
  autolayer(auto_forecast, alpha = 0.3) +
  autolayer(
    auto_forecast,
    series = "Prediction",
    PI = FALSE) +
  ggtitle(
    'Auto Arima Model', 
    subtitle = as.character(auto_fit)) +
  plot_theme

base_plot + 
  autolayer(hyper_forecast, alpha = 0.3) +
  autolayer(
    hyper_forecast,
    series = "Prediction",
    PI = FALSE) +
  ggtitle(
    'Hyperparameter Model', 
    subtitle = as.character(hyper_fit)) +
  plot_theme

base_plot + 
  autolayer(hyper_no_drift_forecast, alpha = 0.3) +
  autolayer(
    hyper_no_drift_forecast,
    series = "Prediction",
    PI = FALSE) +
  ggtitle(
    'Hyperparameter Model w/o Drift', 
    subtitle = as.character(hyper_fit_no_drift)) +
  plot_theme
```

As we can see, the differences between models is minimal, which means we should lean towards selecting the simple arima model with no predictors until we can find some significant attributes that would reduce the error.  Given that the simple model has less than a 9% mean absolute percentage error, it seems as though the decrease in consumption of coal is on a steady decline without any correlated fluctuations to the natural gas consumption.  Therefore, it is unclear if using natural gas consumption as a predictor is useful, but it does not appear to be in this simple model.  If additional feature engineering is performed or other significant predictors are included, then that may remove some of the noise.  In general, there are multiple options to choose from this analysis that will give relatively good predictions, and the selection of one over the other may depend on the purposes for it's future use.  Obviously, other model types could be chosen; however, it seems as though more meaningful predictors would be the key to acheiving higher levels of accuracy.